\documentclass{emulateapj}
%\documentclass[12pt,preprint]{aastex}

\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{epsfig,floatflt}
\usepackage{hyperref}



\begin{document}

\title{A spectacular title}

\author{Daniel Heinesen}

\email{daniel.heinesen@sf-nett.no}

%\altaffiltext{1}{Institute of Theoretical Astrophysics, University of
 % Oslo, P.O.\ Box 1029 Blindern, N-0315 Oslo, Norway}


%\date{Received - / Accepted -}

\begin{abstract}
  State problem. Briefly describe method and data. Summarize main results.
\end{abstract}
\keywords{cosmic microwave background --- cosmology: observations --- methods: statistical}

\section{Introduction}
\label{sec:introduction}

\section{Theory}
\label{sec:theory}
\subsection{The CCD}
\label{sec:ccd}
This is a short qualitative summary of the inner workings of the charge-coupled device (CCD):

A pixel of the CCD consists of three layers: a doped semiconductor layer (usually some silica), separated from a conducting material by some isolator. 

\textbf{INSERT PIC}


The isolating layer is characterized by some energy $E_g$ (g for gap). This gap energy is so that electrons from the semiconducting layer cant overcome it and transverse to the conduction layer. But should a photon hit an electron with higher energy then $E_g$, the electron can overcome the gap energy and reach the conduction layer. The wavelength needed to give the electron this energy is called the cut-off wavelength

\begin{equation}
\lambda_{c} = \frac{hc}{E_g}
\end{equation}

Where $h$ is Planck's constant and $c$ the speed of light. This wavelength determines the largest wavelength the CCD is able to detect. 

After the electrons reach the conduction they are lead to a register, who counts the number of electrons. The number of electrons should be proportional to the number of photons hitting the pixel, and from this the number of photons hitting the pixel can be calculated.


\subsection{Finding the Pixel Size}
To find the pixel size we used a slit to make a diffraction pattern on the screen. We can so use the single-slit diffraction pattern

\begin{equation}
a \sin(\theta) = m\lambda
\label{eq:diffPatt}
\end{equation}

Where $a$ is the width of the slit, $m$ is the order of the minimum, $\theta$ is the angle to said minimum and $\lambda$ is the wavelength of the incoming light/laser.

\emph{INSERT PIC}

If we look at the drawing, we see that the able $\theta$ is given as

\begin{equation}
\tan \theta = \frac{l}{d}
\end{equation}

Where $l$ is the distance to first minimum. But this angle is small, so we can use the small angle approximation $\tan \theta \approx \theta$. The distance $l$ is also not completely known. We know the number of pixels from the center to the first minimum \emph{REF METHODS}, but we do not know how this relate to the actual distance. So we say that

\begin{equation}
l = n_{pixels}\cdot c
\end{equation}

Where $n_{pixels}$ is the number of pixels, and $c$ is some conversion factor, given in meters (the number of pixels are dimensionless). Using this and the small angle approximation we get that

\begin{equation}
\theta \approx \frac{n_{pixels}\cdot c}{d}
\end{equation}

We are also going to use the small angle approximation on \eqref{eq:diffPatt} and use our expression for $\theta$

\begin{equation}
m\lambda \approx a\theta = \frac{n_{pixels}\cdot c}{d}
\end{equation}

This gives us a nice expression for the size of the pixels

\begin{equation}
c \approx \frac{m\lambda d}{n_{pixels}}
\label{eq:pixelSize}
\end{equation}



\subsection{Statistical Analysis}
To clean the images we want ideal flat field, dark frames and bias frames. But this is not the case. There are some noise in the images. This is characterized as the deviation from the mean of the image, or the standard deviation $\sigma$.

\subsubsection{Noise of the Bias}
\label{sec:noiseBias}
For the bias frames we wan to look at just the noise which is not constant. We therefore subtract the two bias frames $B_1 - B_2$. This removes all the constant noise, leaving us with only the undesired variable noise. We can then find the standard deviation of this $\sigma_{B_1 - B_2}$. If we look at the addition rules of variance \emph{REF} we see

\begin{equation}
Var(aX - bY) = a^2Var(X) + b^2Var(b)
\end{equation}

We then get:

\begin{equation}
\sigma_{B_1 - B_2}^2 = \sigma_{B_1}^2 + \sigma_{B_2}^2
\end{equation}

We expect that the noise in the bias frames should be constant over time, so the standard deviation, and thus variance, for the two bias frames should be equal\footnote{Breaking the fourth wall: I'm not sure about this line of reason, but it gives the correct answer, so I included it.}. So

\begin{equation}
\sigma_{B_1 - B_2}^2 = 2\sigma_{B}^2 \Rightarrow \sigma_{B_1 - B_2} = \sqrt{2}\sigma_B
\label{eq:stdB1B2}
\end{equation}

Then thus we see that the standard deviation of the difference of the biases are $\sqrt{2}$ times that of a single bias frame.

We can also look at the mean of the sum of the two bias frames. Since the mean is linear \emph{REF} we get that

\begin{equation}
E(B_1 + B_2) = E(B_1) + E(B_2)
\label{eq:meanB}
\end{equation}

\subsubsection{Noise of the Flat Frames}
\label{sec:noiseFlat}
The same is done with the flat frames. But we want to decrease noise of the flat field, and not increase it as it did in \eqref{eq:stdB1B2}. For that we use several flat fields. So we want to calculate

\begin{equation}
\sigma_{(F_1 + \ldots F_{2n})- (F_2 + \ldots F_{2n+1})}
\end{equation}

Using $n$ pairs of flat fields. But since we have added several frames, the scale of the noise increase with $n$. So we need to normalize with $n$. So what we want to calculate is

\begin{equation}
\frac{\sigma_{(F_1 + \ldots F_{2n})- (F_2 + \ldots F_{2n+1})}}{n}
\end{equation}

If we now use the same relationship as in \eqref{eq:stdB1B2} we see what we want

\begin{equation}
\frac{\sigma_{(F_1 + \ldots F_{2n})- (F_2 + \ldots F_{2n+1})}}{n} = \frac{\sqrt{n}\sigma_F}{n} = \frac{\sigma_F}{\sqrt{n}}
\end{equation}

So we see that as we include more flat frames, the random noise decreases as $1/\sqrt{n}$, and we are left with a more clean flat frame, which can be used to clean our science image.

\subsubsection{Read Out Noise}

We have now found the noise as the standard deviation of the pixel counts in the flat frame. This is in so called analog-to-digital units (ADU). We want to find the readout noise in electrons, so we need a conversion factor. This is given as

\begin{equation}
g = \frac{F_{electrons}}{\sigma_{electrons}} = \frac{(\bar{F}_1 - \bar{F}_2) + (\bar{B}_1-\bar{B}_2)}{\sigma_{F_1 - F_2}^2 - \sigma_{B_1 - B_2}^2}
\end{equation}

Where $F_i$ is a flat frame and $B_i$ is the corresponding bias frame. The second expression comes from the fact that we use the bias frames to clean the flat frames before finding $g$. We can now find the readout noise as

\begin{equation}
\text{R.O.N} = g\cdot\sigma_{bias}
\end{equation}

Which bias frame is used for this should ideally be the same, since we expect time-independent, constant bias noise.

\subsubsection{Cleaning the Image}
To clean the image we want to remove the effects made be the dark current and flat frame. As mentioned \emph{REF} the dark current is additive while the flat field is multiplicative. To remove the noise from the the dark and flat frames, we take the average of multiple frames. But the flat frames do also have dark current, so we need to subtract the the corresponding average dark frame from the average flat frame and normalize it. This is the normalized flat frame. We can then simply clean the raw scientific image of the diffraction pattern by first subtract the average dark frame $D_{I,raw,average}$ and divide by the normalized master flat frame $F_{norm. master}$

\begin{equation}
I_{corrected} = \frac{I_{raw} - D_{I,raw,average}}{F_{norm. master}}
\label{eq:ICorr}
\end{equation}

Where $I_{corrected}$ is the cleaned image of the diffraction pattern.
\section{Method}
\label{sec:method}

\subsection{Color Camera and White Light and Lens}
For this first experiment a white light was shined on and focus by a thin single lens. In the focal point of the lens a microscope objective was placed. The focal point of the lens was fond by holding a piece white paper in front of the lens and finding the distance where the light is most concentrated. 

The CCD was then placed in front of the objective. Here we wanted to look at the properties of the different colors in the white light, so a RGB CCD was used\footnote{For this a Edmund Optics color USB camera was used.}. 

We could now use out Graphical User Interface(GUI) to look at the resulting pattern. The result can be described as a histogram showing the amount of light received be the three sensors in the CCD \emph{REF THEORY}. Due to chromatic aberration \emph{REF TO THEORY} the different colors focus at different focal length, so to look at this effect we moved the CCD carefully back- and forwards to focus the different colors; red, green and blue.

The GUI gave the possibility of viewing the pixel count of a slice of the image, for the different colors. These values were not saved, but the image itself was. The images was then used to make a plot showing the mean pixel count of each color over each column.

The GUI of the CCD also allowed us to adjust the pixel count, frame rate and exposure time, giving us the ability to study the effect this had on the picture.

Filters to filter out some of the colors in the white light was placed in front of the light, so we could look on the effect this would have on the light on the CCD. 


\subsection{Monochromatic Camera and Single Slit}

\subsubsection{Setup}

The white light was now switched out with a monochromatic red laser with wavelength $\lambda = (641 \pm 12.3)$ nm \emph{REF MY SELF}. To ensure that the laser didn't destroy the CCD, a damping filter was placed in front laser. Then a single slit with a width if $a = 100\mu m$ is placed in front of the laser the laser. The diffracted laser light was then captured by a monochromatic CCD\footnote{A monochromatic Edmund Optics USB camera}. 

The camera and slit was then adjusted until a sharp diffraction pattern appeared in the middle of the computer screen. The distance from the slit to the CCD was so measured carefully with a ruler. An image of the diffraction pattern was then taken. To clean the picture the following series of pictures were taken \emph{REF TO THEORY ABOUT CLEANING}. All of the apparatuses were removed, and for all of the pictures except the flat frames (see below) the dust cover was placed on the camera: \\


\begin{itemize}
\item $2$ bias frames with minimum exposer
\item $5$ dark frames with the same exposer as the diffraction pattern
\item $1$ dark frame with maximum exposer
\item $16$ flat frames -- see method below -- with exposer set so that the average pixel value is about half that of max.
\item $5$ dark frames with the same exposer as for the flat frames
\end{itemize}

The flat frames are takes as follows: The dust cover was removed, then a white piece of paper was placed in front of the CCD and a white light was shined on the paper. The white paper ensured a homogeneous, white gradient over the whole CCD chip. With these axillary pictures we could clean the picture of the diffraction pattern.

\subsubsection{Statistical Analysis}
This part was done in Python\emph{REF CODE}, but could be found with most other scripting languages, like R or IDL:

First a bias frame and the maximum exposer dark frames has converted to an array. From these arrays the max, min, mean and distribution of the pixel values was found and compared. \\

To find the readout noise we first looked at the two bias frames. To exclude the possibility of strange results near the edge of the picture, a symmetric $300$x$300$ central part was extracted out of the to images and used. The two  images(the central parts) were then added, and the mean pixel value was found. This is the same value as $\bar{B}_1 + \bar{B}_2$\eqref{eq:meanB}. We then calculated the noise for the biases. This was done by first subtracting the images \ref{sec:noiseBias}, then finding the standard deviation of the resulting image $\sigma_{B_1 - B_2}$\eqref{eq:stdB1B2}.

The same was then done for two flat frames \emph{WRITE WHICH}: first the central regions of the images were found. A mean of the sum of the images was found $\bar{F}_1 + \bar{F}_2$, and then the noise of the difference of the images was found $\sigma_{F_1 - F_2}$.

From this the conversion factor then consequently the readout noise can be calculated \emph{REF THEORY}.\\

To see how the number of flat frames taken improved the normalized noise, the following was calculated with the same approach as the noise above:
$\sigma_{F_1-F_2}$, $\sigma_{(F_1 + F_3) - (F_2+F_4)}$ and so on until all the flat frames were used. For each time a new $\sigma$ as calculated it was normalized with the number of pairs of flat frames used, then saved. The saved values for the noises were then plotted and compared.\\

To clean the image of the diffraction pattern we generally want to subtract away the noise from the dark frames, and divide away the noise from the flat frames. This was done as follows:

First an average of the $16$ flat frames was found $F_{average}$. An average of the $5$ dark frames corresponding to the flat frame $D_{F,average}$ was then found. We then removed the noise in the average flat frame made by the dark current, giving us the master flat frame $F_{master} = F_{average}-D_{F,average}$. This was then normalized with the scalar mean, giving us $F_{norm. master}$.

Secondly the dark frames corresponding to the image of the diffraction pattern was average to find $D_{I,raw,average}$. 

Finally the final, clean image of the diffraction pattern was given as \emph{REF THOERY}

\begin{equation}
I_{corrected} = \frac{I_{raw} - D_{I,raw,average}}{F_{norm. master}}
\end{equation}

This was done both for the central 300x300 image and the full field of view.

With a clean image of the diffraction pattern, the values of the middle of the images was plotted, and the distance, in pixels, to the first minima was found. From this distance the size of the pixels could be found \eqref{eq:pixelSize}. 

\subsection{Uncertainties}

\subsubsection{Color Camera and White Light and Lens}
To get a sharp diffraction pattern, the lens, objective and CCD has to be normal to the direction of the path of the light. If the lens is off the normal, we will get a coma, and due to the chromatic aberration in the lens (even thou it is thin, it has a physical thickness) the coma will be enhanced, leading to a smear out of colors(as seen here \emph{REF PIC}). This means that it might be difficult to get a sharp pattern, and the Airy disk might be larger then expected due to this coma, unless the lens is completely normal to the light(fortunately we are not measuring the Airy disk, so this uncertainty is only a curiosity).

When looking at the pixel counts for the different colors, we wanted to look at the pixel count when green dominated. But since green is a mix of blue and yellow, the blue is quite high when ever green is high. So there is quite a small range where green dominates all other colors, including blue. This range was difficult to find by only moving the camera back and forth, so the resulting image does not show very good results.

\subsubsection{Monochromatic Camera and Single Slit}

Much like with the white light and lens, the quality of the diffraction pattern is dependent on the single slit, objective and CCD being normal to the laser. This was adjusted as best as possible to make this uncertainty as low as possible.
The slit has imperfections in its construction, making the diffraction pattern very uneven, instead of the straight lines we ideally want to see \emph{REF IMAGE}.
The dominating source of error in our calculations of the pixel size came from the way the number of pixels to the first minimum was found: as one can see in the image of the diffraction pattern \emph{REF IMAGE} is tilted at an angle, while the plotted slice of the image \emph{REF PLOT} is take horizontally, meaning that the measured distance to the minimum is larger than the actual distance.
All of these uncertainties has been encompassed by a large, but fair uncertainty of $\pm 5$ pixels. 

The distance from the slit to the camera was measured with a ruler. There was difficulty measuring this distance due to both the exact position of the slit and that of the CCD chip being obscured by their respective casings. So an uncertainty of $\pm 0.2$ cm was added.\\

As will be discussed later \emph{REF DISCUSSION} the way we made the flat field was at best lacking. The bends in the paper, especially at the edges would lead to parts of the paper being darker than the uniform gradient we are looking for. But most grievous error done was taking the flat field without any slit or objective, as will be discussed later.




 

\section{Data}
\label{sec:data}



\section{Results}
\label{sec:results}

\section{Conclusions}
\label{sec:conclusions}




\end{document}